{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LskE4B53rUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2cbda3-c23e-4bda-c86d-c0b13177d7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#graph\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras import activations, initializers, constraints\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import Layer\n",
        "#from keras import Layer\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "import keras\n",
        "\n",
        "\n",
        "class GraphLayer(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 step_num=1,\n",
        "                 activation=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initialize the layer.\n",
        "\n",
        "        :param step_num: Two nodes are considered as connected if they could be reached in `step_num` steps.\n",
        "        :param activation: The activation function after convolution.\n",
        "        :param kwargs: Other arguments for parent class.\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.step_num = step_num\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.supports_masking = True\n",
        "        super(GraphLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'step_num': self.step_num,\n",
        "            'activation': self.activation,\n",
        "        }\n",
        "        base_config = super(GraphLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def _get_walked_edges(self, edges, step_num):\n",
        "        \"\"\"Get the connection graph within `step_num` steps\n",
        "\n",
        "        :param edges: The graph in single step.\n",
        "        :param step_num: Number of steps.\n",
        "        :return: The new graph that has the same shape with `edges`.\n",
        "        \"\"\"\n",
        "        if step_num <= 1:\n",
        "            return edges\n",
        "        deeper = self._get_walked_edges(K.batch_dot(edges, edges), step_num // 2)\n",
        "        if step_num % 2 == 1:\n",
        "            deeper += edges\n",
        "        return K.cast(K.greater(deeper, 0.0), K.floatx())\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        features, edges = inputs\n",
        "        edges = K.cast(edges, K.floatx())\n",
        "        if self.step_num > 1:\n",
        "            edges = self._get_walked_edges(edges, self.step_num)\n",
        "        outputs = self.activation(self._call(features, edges))\n",
        "        return outputs\n",
        "\n",
        "    def _call(self, features, edges):\n",
        "        raise NotImplementedError('The class is not intended to be used directly.')\n",
        "\n",
        "\n",
        "class GraphConv(GraphLayer):\n",
        "    \"\"\"Graph convolutional layer.\n",
        "\n",
        "    h_i^{(t)} = \\sigma \\left ( \\frac{ G_i^T (h_i^{(t - 1)} W + b)}{\\sum G_i}  \\right )\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 use_bias=True,\n",
        "                 bias_initializer='zeros',\n",
        "                 bias_regularizer=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initialize the layer.\n",
        "\n",
        "        :param units: Number of new states. If the input shape is (batch_size, node_num, feature_len), then the output\n",
        "                      shape is (batch_size, node_num, units).\n",
        "        :param kernel_initializer: The initializer of the kernel weight matrix.\n",
        "        :param kernel_regularizer: The regularizer of the kernel weight matrix.\n",
        "        :param kernel_constraint:  The constraint of the kernel weight matrix.\n",
        "        :param use_bias: Whether to use bias term.\n",
        "        :param bias_initializer: The initializer of the bias vector.\n",
        "        :param bias_regularizer: The regularizer of the bias vector.\n",
        "        :param bias_constraint: The constraint of the bias vector.\n",
        "        :param kwargs: Other arguments for parent class.\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
        "        self.use_bias = use_bias\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
        "\n",
        "        self.W, self.b = None, None\n",
        "        super(GraphConv, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
        "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
        "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
        "            'use_bias': self.use_bias,\n",
        "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
        "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
        "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
        "        }\n",
        "        base_config = super(GraphConv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        feature_dim = input_shape[0][2]\n",
        "        self.W = self.add_weight(\n",
        "            shape=(feature_dim, self.units),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            name='{}_W'.format(self.name),\n",
        "        )\n",
        "        if self.use_bias:\n",
        "            self.b = self.add_weight(\n",
        "                shape=(self.units,),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                name='{}_b'.format(self.name),\n",
        "            )\n",
        "        super(GraphConv, self).build(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0][:2] + (self.units,)\n",
        "\n",
        "   # def compute_mask(self, inputs, mask=None):\n",
        "       # return mask[0]\n",
        "\n",
        "    def _call(self, features, edges):\n",
        "        features = K.dot(features, self.W)\n",
        "        if self.use_bias:\n",
        "            features += self.b\n",
        "        if self.step_num > 1:\n",
        "            edges = self._get_walked_edges(edges, self.step_num)\n",
        "        return K.batch_dot(K.permute_dimensions(edges, (0, 2, 1)), features) #\\\n",
        "           # / (K.sum(edges, axis=2, keepdims=True) + K.epsilon())\n",
        "\n",
        "\n",
        "class GraphPool(GraphLayer):\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask[0]\n",
        "\n",
        "\n",
        "class GraphMaxPool(GraphPool):\n",
        "\n",
        "    NEG_INF = -1e38\n",
        "\n",
        "    def _call(self, features, edges):\n",
        "        node_num = K.shape(features)[1]\n",
        "        features = K.tile(K.expand_dims(features, axis=1), K.stack([1, node_num, 1, 1])) \\\n",
        "            + K.expand_dims((1.0 - edges) * self.NEG_INF, axis=-1)\n",
        "        return K.max(features, axis=2)\n",
        "\n",
        "\n",
        "class GraphAveragePool(GraphPool):\n",
        "\n",
        "    def _call(self, features, edges):\n",
        "        return K.batch_dot(K.permute_dimensions(edges, (0, 2, 1)), features) \\\n",
        "            / (K.sum(edges, axis=2, keepdims=True) + K.epsilon())\n"
      ],
      "metadata": {
        "id": "Ikai-Wn63wjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utils\n",
        "from __future__ import print_function\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "!pip install --upgrade scipy\n",
        "!pip uninstall scipy\n",
        "!pip install scipy\n",
        "from scipy.sparse.linalg.eigen import eigs ,eigsh, ArpackNoConvergence\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
        "\n",
        "    return features.todense(), adj, labels\n",
        "\n",
        "\n",
        "def normalize_adj(adj, symmetric=True):\n",
        "    if symmetric:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0).toarray()\n",
        "        a_norm = adj.dot(d).transpose().dot(d)\n",
        "    else:\n",
        "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0).toarray()\n",
        "        a_norm = d.dot(adj)\n",
        "    return a_norm\n",
        "\n",
        "\n",
        "def preprocess_adj(adj, symmetric=True):\n",
        "    adj = adj + np.eye(adj.shape[0])\n",
        "    adj = normalize_adj(adj, symmetric)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def sample_mask(idx, l):\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def get_splits(y):\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
        "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
        "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
        "    y_train[idx_train] = y[idx_train]\n",
        "    y_val[idx_val] = y[idx_val]\n",
        "    y_test[idx_test] = y[idx_test]\n",
        "    train_mask = sample_mask(idx_train, y.shape[0])\n",
        "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask\n",
        "\n",
        "\n",
        "def categorical_crossentropy(preds, labels):\n",
        "    return np.mean(-np.log(np.extract(labels, preds)))\n",
        "\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
        "\n",
        "\n",
        "def evaluate_preds(preds, labels, indices):\n",
        "\n",
        "    split_loss = list()\n",
        "    split_acc = list()\n",
        "\n",
        "    for y_split, idx_split in zip(labels, indices):\n",
        "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
        "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
        "\n",
        "    return split_loss, split_acc\n",
        "\n",
        "\n",
        "def normalized_laplacian(adj, symmetric=True):\n",
        "    adj_normalized = normalize_adj(adj, symmetric)\n",
        "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
        "    return laplacian\n",
        "\n",
        "\n",
        "def rescale_laplacian(laplacian):\n",
        "    try:\n",
        "        print('Calculating largest eigenvalue of normalized graph Laplacian...')\n",
        "        largest_eigval = eigsh(laplacian, 1, which='LM', return_eigenvectors=False)[0]\n",
        "    except ArpackNoConvergence:\n",
        "        print('Eigenvalue calculation did not converge! Using largest_eigval=2 instead.')\n",
        "        largest_eigval = 2\n",
        "\n",
        "    scaled_laplacian = (2. / largest_eigval) * laplacian - sp.eye(laplacian.shape[0])\n",
        "    return scaled_laplacian\n",
        "\n",
        "\n",
        "def chebyshev_polynomial(X, k):\n",
        "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices.\"\"\"\n",
        "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
        "\n",
        "    T_k = list()\n",
        "    T_k.append(sp.eye(X.shape[0]).tocsr())\n",
        "    T_k.append(X)\n",
        "\n",
        "    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):\n",
        "        X_ = sp.csr_matrix(X, copy=True)\n",
        "        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two\n",
        "\n",
        "    for i in range(2, k+1):\n",
        "        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))\n",
        "\n",
        "    return T_k\n",
        "\n",
        "\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape"
      ],
      "metadata": {
        "id": "ohRdHVxC3yhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c17a93-11f4-4bb6-9bcb-822a31a1cea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy-1.10.1.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/scipy/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-b4ad1f0684ec>:9: DeprecationWarning: Please use `eigs` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
            "  from scipy.sparse.linalg.eigen import eigs ,eigsh, ArpackNoConvergence\n",
            "<ipython-input-3-b4ad1f0684ec>:9: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
            "  from scipy.sparse.linalg.eigen import eigs ,eigsh, ArpackNoConvergence\n",
            "<ipython-input-3-b4ad1f0684ec>:9: DeprecationWarning: Please use `ArpackNoConvergence` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
            "  from scipy.sparse.linalg.eigen import eigs ,eigsh, ArpackNoConvergence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input,InputLayer,Multiply,ZeroPadding2D\n",
        "from keras.layers import Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\n",
        "from keras.layers import Dense,Activation,Dropout,Flatten,Concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Dropout,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "#from layers.graph import GraphLayer,GraphConv\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.losses import categorical_hinge\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class KerasMultiSourceGCNModel(object):\n",
        "    def __init__(self,use_mut,use_gexp,use_methy,regr=True):#\n",
        "        self.use_mut = use_mut\n",
        "        self.use_gexp = use_gexp\n",
        "        self.use_methy = use_methy\n",
        "        self.regr = regr\n",
        "    def createMaster(self,drug_dim,mutation_dim,gexpr_dim,methy_dim,units_list,use_relu=True,use_bn=True,use_GMP=True):\n",
        "        drug_feat_input = Input(shape=(None,drug_dim),name='drug_feat_input')#drug_dim=75\n",
        "        drug_adj_input = Input(shape=(None,None),name='drug_adj_input')\n",
        "\n",
        "        mutation_input = Input(shape=(1,mutation_dim,1),name='mutation_feat_input')\n",
        "        gexpr_input = Input(shape=(gexpr_dim,),name='gexpr_feat_input')\n",
        "        methy_input = Input(shape=(methy_dim,),name='methy_feat_input')\n",
        "        #drug feature with GCN\n",
        "        GCN_layer = GraphConv(units=units_list[0],step_num=1)([drug_feat_input,drug_adj_input])\n",
        "        if use_relu:\n",
        "            GCN_layer = Activation('relu')(GCN_layer)\n",
        "        else:\n",
        "            GCN_layer = Activation('tanh')(GCN_layer)\n",
        "        if use_bn:\n",
        "            GCN_layer = BatchNormalization()(GCN_layer)\n",
        "        GCN_layer = Dropout(0.1)(GCN_layer)\n",
        "\n",
        "        for i in range(len(units_list)-1):\n",
        "            GCN_layer = GraphConv(units=units_list[i+1],step_num=1)([GCN_layer,drug_adj_input])\n",
        "            if use_relu:\n",
        "                GCN_layer = Activation('relu')(GCN_layer)\n",
        "            else:\n",
        "                GCN_layer = Activation('tanh')(GCN_layer)\n",
        "            if use_bn:\n",
        "                GCN_layer = BatchNormalization()(GCN_layer)\n",
        "            GCN_layer = Dropout(0.1)(GCN_layer)\n",
        "\n",
        "        GCN_layer = GraphConv(units=100,step_num=1)([GCN_layer,drug_adj_input])\n",
        "        if use_relu:\n",
        "            GCN_layer = Activation('relu')(GCN_layer)\n",
        "        else:\n",
        "            GCN_layer = Activation('tanh')(GCN_layer)\n",
        "        if use_bn:\n",
        "            GCN_layer = BatchNormalization()(GCN_layer)\n",
        "        GCN_layer = Dropout(0.1)(GCN_layer)\n",
        "        #global pooling\n",
        "        if use_GMP:\n",
        "            x_drug = GlobalMaxPooling1D()(GCN_layer)\n",
        "        else:\n",
        "            x_drug = GlobalAveragePooling1D()(GCN_layer)\n",
        "\n",
        "        #genomic mutation feature\n",
        "        x_mut = Conv2D(filters=50, kernel_size=(1,700),strides=(1, 5), activation = 'tanh',padding='valid')(mutation_input)\n",
        "        x_mut = MaxPooling2D(pool_size=(1,5))(x_mut)\n",
        "        x_mut = Conv2D(filters=30, kernel_size=(1,5),strides=(1, 2), activation = 'relu',padding='valid')(x_mut)\n",
        "        x_mut = MaxPooling2D(pool_size=(1,10))(x_mut)\n",
        "        x_mut = Flatten()(x_mut)\n",
        "        x_mut = Dense(100,activation = 'relu')(x_mut)\n",
        "        x_mut = Dropout(0.1)(x_mut)\n",
        "        #gexp feature\n",
        "        x_gexpr = Dense(256)(gexpr_input)\n",
        "        x_gexpr = Activation('tanh')(x_gexpr)\n",
        "        x_gexpr = BatchNormalization()(x_gexpr)\n",
        "        x_gexpr = Dropout(0.1)(x_gexpr)\n",
        "        x_gexpr = Dense(100,activation='relu')(x_gexpr)\n",
        "        #methylation feature\n",
        "        x_methy = Dense(256)(methy_input)\n",
        "        x_methy = Activation('tanh')(x_methy)\n",
        "        x_methy = BatchNormalization()(x_methy)\n",
        "        x_methy = Dropout(0.1)(x_methy)\n",
        "        x_methy = Dense(100,activation='relu')(x_methy)\n",
        "        x = x_drug\n",
        "        if self.use_mut:\n",
        "            x = Concatenate()([x,x_mut])\n",
        "        if self.use_gexp:\n",
        "            x = Concatenate()([x,x_gexpr])\n",
        "        if self.use_methy:\n",
        "            x = Concatenate()([x,x_methy])\n",
        "        #x = Concatenate()([x_mut,x_drug,x_gexpr,x_methy])\n",
        "        x = Dense(300,activation = 'tanh')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Lambda(lambda x: K.expand_dims(x,axis=-1))(x)\n",
        "        x = Lambda(lambda x: K.expand_dims(x,axis=1))(x)\n",
        "        x = Conv2D(filters=30, kernel_size=(1,150),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
        "        x = MaxPooling2D(pool_size=(1,2))(x)\n",
        "        x = Conv2D(filters=10, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
        "        x = MaxPooling2D(pool_size=(1,3))(x)\n",
        "        x = Conv2D(filters=5, kernel_size=(1,5),strides=(1, 1), activation = 'relu',padding='valid')(x)\n",
        "        x = MaxPooling2D(pool_size=(1,3))(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Flatten()(x)\n",
        "        def custom_loss_value(y_true, y_pred):\n",
        "              x = K.eval(y_pred)\n",
        "              print(X)\n",
        "              Y = np.ravel(K.eval(y_true))\n",
        "              Predict = []\n",
        "              Prob = []\n",
        "              scaler = StandardScaler()\n",
        "              X = scaler.fit_transform(X)\n",
        "              param_grid = {'C': [0.1, 1, 8, 10], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
        "              reg = LinearRegression().fit(X, Y)\n",
        "              Final_Model = reg.best_estimator_\n",
        "              Predict = Final_Model.predict(X)\n",
        "              Prob = Final_Model.predict_proba(X)\n",
        "              return categorical_hinge(tf.convert_to_tensor(Y, dtype=tf.float32), tf.convert_to_tensor(Predict, dtype=tf.float32))\n",
        "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        x = Dropout(0.2)(x)\n",
        "        if self.regr:\n",
        "            output = Dense(1,name='output')(x)\n",
        "        else:\n",
        "            output = Dense(1,activation = 'sigmoid',name='output')(x)\n",
        "        model  = Model(inputs=[drug_feat_input,drug_adj_input,mutation_input,gexpr_input,methy_input],outputs=output)\n",
        "        model.compile(loss=custom_loss_value, optimizer=sgd, metrics=['accuracy'])\n",
        "        return model\n",
        "\n"
      ],
      "metadata": {
        "id": "i9Da0M7x30y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process drug\n",
        "#get drug features using Deepchem library\n",
        "!pip install hickle\n",
        "!pip install tensorflow-gpu\n",
        "!pip install parse\n",
        "!pip install deepchem\n",
        "import deepchem as dc\n",
        "#!pip install --upgrade scipy\n",
        "#!pip uninstall scipy\n",
        "#!pip install scipy\n",
        "\n",
        "import os\n",
        "\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import hickle as hkl\n",
        "\n",
        "\n",
        "drug_smiles_file='/content/drive/MyDrive/Deep_CDR/223drugs_pubchem_smiles.txt'\n",
        "save_dir='/content/drive/MyDrive/Deep_CDR/GDSC/drug_graph_feat'\n",
        "pubchemid2smile = {item.split('\\t')[0]:item.split('\\t')[1].strip() for item in open(drug_smiles_file).readlines()}\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "molecules = []\n",
        "for each in pubchemid2smile.keys():\n",
        "\tprint(each)\n",
        "\tmolecules=[]\n",
        "\tmolecules.append(Chem.MolFromSmiles(pubchemid2smile[each]))\n",
        "\tfeaturizer = dc.feat.graph_features.ConvMolFeaturizer()\n",
        "\tmol_object = featurizer.featurize(molecules)\n",
        "\tfeatures = mol_object[0].atom_features\n",
        "\tdegree_list = mol_object[0].deg_list\n",
        "\tadj_list = mol_object[0].canon_adj_list\n",
        "\thkl.dump([features,adj_list,degree_list],'%s/%s.hkl'%(save_dir,each))\n"
      ],
      "metadata": {
        "id": "ZvhA0FhT353W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c1ce6fe-1036-4fb2-f411-b20ca31b4a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hickle\n",
            "  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle) (3.8.0)\n",
            "Requirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.10/dist-packages (from hickle) (1.22.4)\n",
            "Installing collected packages: hickle\n",
            "Successfully installed hickle-5.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: parse\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24570 sha256=89658268dcd5eca2b5ce595bd25a74cad25b9e220805538d6cbcfb6a31afab91\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4b/f0/eaf5a8de646d8676dc25caa01949b9f9d883b8fa2efb435bc3\n",
            "Successfully built parse\n",
            "Installing collected packages: parse\n",
            "Successfully installed parse-1.19.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2022.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (8.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, rdkit, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed deepchem-2.7.1 rdkit-2023.3.1 scipy-1.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.models.torch_models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.10/dist-packages/deepchem/models/torch_models/__init__.py)\n",
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25102847\n",
            "148124\n",
            "5289247\n",
            "9868037\n",
            "11364421\n",
            "208908\n",
            "4993\n",
            "462382\n",
            "57339144\n",
            "2375\n",
            "448013\n",
            "24756910\n",
            "24785538\n",
            "637858\n",
            "11624601\n",
            "7251185\n",
            "6445533\n",
            "11373846\n",
            "6918638\n",
            "23725625\n",
            "46843772\n",
            "24889392\n",
            "387447\n",
            "11617559\n",
            "24772860\n",
            "10302451\n",
            "5394\n",
            "31703\n",
            "44551660\n",
            "6710780\n",
            "24776445\n",
            "6445562\n",
            "521106\n",
            "5327091\n",
            "78243717\n",
            "46907787\n",
            "82146\n",
            "10184653\n",
            "9874913\n",
            "6852167\n",
            "60750\n",
            "44819241\n",
            "16663089\n",
            "11404337\n",
            "46943432\n",
            "5311510\n",
            "3062316\n",
            "11640390\n",
            "6918454\n",
            "9967941\n",
            "9911830\n",
            "49806720\n",
            "24180719\n",
            "42642645\n",
            "16760646\n",
            "44462760\n",
            "5311497\n",
            "3796\n",
            "11676786\n",
            "17755052\n",
            "25022668\n",
            "36462\n",
            "36314\n",
            "10451420\n",
            "444795\n",
            "10077147\n",
            "6914657\n",
            "11844351\n",
            "44143370\n",
            "11667893\n",
            "46930998\n",
            "25257557\n",
            "6918289\n",
            "3385\n",
            "1401\n",
            "10074640\n",
            "5278396\n",
            "76044\n",
            "5329102\n",
            "5459322\n",
            "300471\n",
            "176158\n",
            "20635522\n",
            "25222038\n",
            "16725726\n",
            "9952773\n",
            "5113032\n",
            "156422\n",
            "644215\n",
            "16095342\n",
            "9826308\n",
            "85668777\n",
            "25262965\n",
            "71271629\n",
            "176870\n",
            "11634725\n",
            "11282283\n",
            "9938202\n",
            "9826528\n",
            "11152667\n",
            "704473\n",
            "46844147\n",
            "5746\n",
            "560326\n",
            "49821040\n",
            "11977753\n",
            "54676905\n",
            "65110\n",
            "49836027\n",
            "11433190\n",
            "8249\n",
            "11609586\n",
            "5291\n",
            "10390396\n",
            "25124816\n",
            "216239\n",
            "11455910\n",
            "5311\n",
            "16038120\n",
            "3463933\n",
            "4263900\n",
            "5280757\n",
            "11713159\n",
            "9956222\n",
            "10096043\n",
            "6450551\n",
            "25167777\n",
            "46224516\n",
            "5460769\n",
            "5717801\n",
            "11493598\n",
            "123631\n",
            "56965967\n",
            "216326\n",
            "42640\n",
            "6753378\n",
            "56962337\n",
            "5328940\n",
            "2726824\n",
            "9910224\n",
            "44632017\n",
            "3005532\n",
            "53394750\n",
            "66577006\n",
            "9813758\n",
            "10459196\n",
            "46883536\n",
            "176167\n",
            "54685215\n",
            "6450816\n",
            "11960529\n",
            "10027278\n",
            "9858940\n",
            "11707110\n",
            "2314623\n",
            "11178236\n",
            "9943465\n",
            "44137675\n",
            "9907093\n",
            "11647372\n",
            "4261\n",
            "9903786\n",
            "9863776\n",
            "2733526\n",
            "11327430\n",
            "5208\n",
            "11626560\n",
            "24951314\n",
            "46885626\n",
            "3218\n",
            "44450571\n",
            "24826799\n",
            "11485656\n",
            "46191454\n",
            "6505803\n",
            "126941\n",
            "24825971\n",
            "5494449\n",
            "10384072\n",
            "9810884\n",
            "9914412\n",
            "11626927\n",
            "53302361\n",
            "447912\n",
            "16720766\n",
            "16747388\n",
            "10109823\n",
            "5330286\n",
            "44224160\n",
            "9931953\n",
            "9884685\n",
            "11338033\n",
            "10200390\n",
            "25126798\n",
            "16722836\n",
            "10113978\n",
            "24894414\n",
            "9956119\n",
            "10172943\n",
            "53340664\n",
            "11316960\n",
            "25195352\n",
            "6918848\n",
            "84691\n",
            "46931012\n",
            "10341154\n",
            "44182295\n",
            "644241\n",
            "51371303\n",
            "11625818\n",
            "11754511\n",
            "10127622\n",
            "159324\n",
            "5384616\n",
            "160355\n",
            "9549184\n",
            "104842\n",
            "10196499\n",
            "24978538\n",
            "10461815\n",
            "126565\n",
            "446378\n",
            "6253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run_deep_CDR\n",
        "import json\n",
        "\n",
        "import argparse\n",
        "import random,os,sys\n",
        "import numpy as np\n",
        "import csv\n",
        "#!pip install --upgrade scipy\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input,InputLayer,Multiply,ZeroPadding2D\n",
        "from keras.layers import Conv2D, AveragePooling2D\n",
        "from keras.layers import Dense,Activation,Dropout,Flatten,Concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Lambda\n",
        "from keras import optimizers,utils\n",
        "from keras.constraints import max_norm\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint,Callback,EarlyStopping,History\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import model_from_json\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import average_precision_score\n",
        "from scipy.stats import pearsonr\n",
        "#from model import KerasMultiSourceGCNModel\n",
        "import hickle as hkl\n",
        "import scipy.sparse as sp\n",
        "import argparse\n",
        "\n",
        "####################################Settings#################################\n",
        "parser = argparse.ArgumentParser(description='Drug_response_pre')\n",
        "parser.add_argument('-gpu_id', dest='gpu_id', type=str, default='0', help='GPU devices')\n",
        "parser.add_argument('-use_mut', dest='use_mut', type=bool, default=True, help='use gene mutation or not')\n",
        "parser.add_argument('-use_gexp', dest='use_gexp', type=bool, default=True, help='use gene expression or not')\n",
        "parser.add_argument('-use_methy', dest='use_methy', type=bool, default=True, help='use methylation or not')\n",
        "\n",
        "parser.add_argument('-israndom', dest='israndom', type=bool, default=False, help='randomlize X and A')\n",
        "\n",
        "#hyparameters for GCN\n",
        "\n",
        "parser.add_argument('-unit_list', dest='unit_list', nargs='+', type=int, default=[256,256,256],help='unit list for GCN')\n",
        "parser.add_argument('-use_bn', dest='use_bn', type=bool, default=True, help='use batchnormalization for GCN')\n",
        "parser.add_argument('-use_relu', dest='use_relu', type=bool, default=True, help='use relu for GCN')\n",
        "parser.add_argument('-use_GMP', dest='use_GMP', type=bool, help='use GlobalAveragePooling for GCN')\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n",
        "use_mut,use_gexp,use_methy = args.use_mut,args.use_gexp, args.use_methy\n",
        "israndom=args.israndom\n",
        "model_suffix = ('with_mut' if use_mut else 'without_mut')+'_'+('with_gexp' if use_gexp else 'without_gexp')+'_'+('with_methy' if use_methy else 'without_methy')\n",
        "\n",
        "GCN_deploy = '_'.join(map(str,args.unit_list)) + '_'+('bn' if args.use_bn else 'no_bn')+'_'+('relu' if args.use_relu else 'tanh')+'_'+('GMP' if args.use_GMP else 'GAP')\n",
        "model_suffix = model_suffix + '_' +GCN_deploy\n",
        "\n",
        "\n",
        "####################################Constants Settings###########################\n",
        "TCGA_label_set = [\"ALL\",\"BLCA\",\"BRCA\",\"CESC\",\"DLBC\",\"LIHC\",\"LUAD\",\n",
        "                  \"ESCA\",\"GBM\",\"HNSC\",\"KIRC\",\"LAML\",\"LCML\",\"LGG\",\n",
        "                  \"LUSC\",\"MESO\",\"MM\",\"NB\",\"OV\",\"PAAD\",\"SCLC\",\"SKCM\",\n",
        "                  \"STAD\",\"THCA\",'COAD/READ']\n",
        "DPATH = '../data'\n",
        "Drug_info_file = '/content/drive/MyDrive/Deep_CDR/GDSC/1.Drug_listMon Jun 24 09_00_55 2019.csv'\n",
        "Cell_line_info_file = '/content/drive/MyDrive/Deep_CDR/CCLE/Cell_lines_annotations_20181226.txt'\n",
        "Drug_feature_file = '/content/drive/MyDrive/Deep_CDR/GDSC/drug_graph_feat'\n",
        "Genomic_mutation_file = '/content/drive/MyDrive/Deep_CDR/CCLE/genomic_mutation_34673_demap_features.csv'\n",
        "Cancer_response_exp_file = '/content/drive/MyDrive/Deep_CDR/CCLE/GDSC_IC50.csv'\n",
        "Gene_expression_file = '/content/drive/MyDrive/Deep_CDR/CCLE/genomic_expression_561celllines_697genes_demap_features.csv'\n",
        "Methylation_file = '/content/drive/MyDrive/Deep_CDR/CCLE/genomic_methylation_561celllines_808genes_demap_features.csv'\n",
        "Max_atoms = 100\n",
        "\n",
        "def MetadataGenerate(Drug_info_file,Cell_line_info_file,Genomic_mutation_file,Drug_feature_file,Gene_expression_file,Methylation_file,filtered):\n",
        "    #drug_id --> pubchem_id\n",
        "    reader = csv.reader(open('/content/drive/MyDrive/Deep_CDR/GDSC/1.Drug_listMon Jun 24 09_00_55 2019.csv','r'))\n",
        "    rows = [item for item in reader]\n",
        "    drugid2pubchemid = {item[0]:item[5] for item in rows if item[5].isdigit()}\n",
        "\n",
        "    #map cellline --> cancer type\n",
        "    cellline2cancertype ={}\n",
        "    for line in open(Cell_line_info_file).readlines()[1:]:\n",
        "        cellline_id = line.split('\\t')[1]\n",
        "        TCGA_label = line.strip().split('\\t')[-1]\n",
        "        #if TCGA_label in TCGA_label_set:\n",
        "        cellline2cancertype[cellline_id] = TCGA_label\n",
        "\n",
        " #load demap cell lines genomic mutation features\n",
        "    mutation_feature = pd.read_csv(Genomic_mutation_file,sep=',',header=0,index_col=[0])\n",
        "    cell_line_id_set = list(mutation_feature.index)\n",
        "\n",
        "    # load drug features\n",
        "    drug_pubchem_id_set = []\n",
        "    drug_feature = {}\n",
        "    for each in os.listdir(Drug_feature_file):\n",
        "        drug_pubchem_id_set.append(each.split('.')[0])\n",
        "        feat_mat,adj_list,degree_list = hkl.load('%s/%s'%(Drug_feature_file,each))\n",
        "        drug_feature[each.split('.')[0]] = [feat_mat,adj_list,degree_list]\n",
        "    assert len(drug_pubchem_id_set)==len(drug_feature.values())\n",
        "\n",
        "    #load gene expression faetures\n",
        "    gexpr_feature = pd.read_csv(Gene_expression_file,sep=',',header=0,index_col=[0])\n",
        "\n",
        "    #only keep overlapped cell lines\n",
        "    mutation_feature = mutation_feature.loc[list(gexpr_feature.index)]\n",
        "\n",
        "    #load methylation\n",
        "    methylation_feature = pd.read_csv(Methylation_file,sep=',',header=0,index_col=[0])\n",
        "   # assert methylation_feature.shape[0]==gexpr_feature.shape[0]==mutation_feature.shape[0]\n",
        "    experiment_data = pd.read_csv(Cancer_response_exp_file,sep=',',header=0,index_col=[0])\n",
        "    #filter experiment data\n",
        "    drug_match_list=[item for item in experiment_data.index if item.split(':')[1] in drugid2pubchemid.keys()]\n",
        "    experiment_data_filtered = experiment_data.loc[drug_match_list]\n",
        "\n",
        "    data_idx = []\n",
        "    for each_drug in experiment_data_filtered.index:\n",
        "        for each_cellline in experiment_data_filtered.columns:\n",
        "            pubchem_id = drugid2pubchemid[each_drug.split(':')[-1]]\n",
        "            if str(pubchem_id) in drug_pubchem_id_set and each_cellline in mutation_feature.index:\n",
        "                if not np.isnan(experiment_data_filtered.loc[each_drug,each_cellline]) and each_cellline in cellline2cancertype.keys():\n",
        "                    ln_IC50 = float(experiment_data_filtered.loc[each_drug,each_cellline])\n",
        "                    data_idx.append((each_cellline,pubchem_id,ln_IC50,cellline2cancertype[each_cellline]))\n",
        "    nb_celllines = len(set([item[0] for item in data_idx]))\n",
        "    nb_drugs = len(set([item[1] for item in data_idx]))\n",
        "    print('%d instances across %d cell lines and %d drugs were generated.'%(len(data_idx),nb_celllines,nb_drugs))\n",
        "    return mutation_feature, drug_feature,gexpr_feature,methylation_feature, data_idx\n",
        "#split into training and test set\n",
        "def DataSplit(data_idx,ratio = 0.80):\n",
        "    data_train_idx,data_test_idx = [], []\n",
        "    for each_type in TCGA_label_set:\n",
        "        data_subtype_idx = [item for item in data_idx if item[-1]==each_type]\n",
        "        train_list = random.sample(data_subtype_idx,int(ratio*len(data_subtype_idx)))\n",
        "        test_list = [item for item in data_subtype_idx if item not in train_list]\n",
        "        data_train_idx += train_list\n",
        "        data_test_idx += test_list\n",
        "    return data_train_idx,data_test_idx\n",
        "\n",
        "def NormalizeAdj(adj):\n",
        "    adj = adj + np.eye(adj.shape[0])\n",
        "    d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0).toarray()\n",
        "    a_norm = adj.dot(d).transpose().dot(d)\n",
        "    return a_norm\n",
        "def random_adjacency_matrix(n):\n",
        "    matrix = [[random.randint(0, 1) for i in range(n)] for j in range(n)]\n",
        "    # No vertex connects to itself\n",
        "    for i in range(n):\n",
        "        matrix[i][i] = 0\n",
        "    # If i is connected to j, j is connected to i\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            matrix[j][i] = matrix[i][j]\n",
        "    return matrix\n",
        "\n",
        "def CalculateGraphFeat(feat_mat,adj_list):\n",
        "    assert feat_mat.shape[0] == len(adj_list)\n",
        "    feat = np.zeros((Max_atoms,feat_mat.shape[-1]),dtype='float32')\n",
        "    adj_mat = np.zeros((Max_atoms,Max_atoms),dtype='float32')\n",
        "    if israndom:\n",
        "        feat = np.random.rand(Max_atoms,feat_mat.shape[-1])\n",
        "        adj_mat[feat_mat.shape[0]:,feat_mat.shape[0]:] = random_adjacency_matrix(Max_atoms-feat_mat.shape[0])\n",
        "    feat[:feat_mat.shape[0],:] = feat_mat\n",
        "    for i in range(len(adj_list)):\n",
        "        nodes = adj_list[i]\n",
        "        for each in nodes:\n",
        "            adj_mat[i,int(each)] = 1\n",
        "    assert np.allclose(adj_mat,adj_mat.T)\n",
        "    adj_ = adj_mat[:len(adj_list),:len(adj_list)]\n",
        "    adj_2 = adj_mat[len(adj_list):,len(adj_list):]\n",
        "    norm_adj_ = NormalizeAdj(adj_)\n",
        "    norm_adj_2 = NormalizeAdj(adj_2)\n",
        "    adj_mat[:len(adj_list),:len(adj_list)] = norm_adj_\n",
        "    adj_mat[len(adj_list):,len(adj_list):] = norm_adj_2\n",
        "    return [feat,adj_mat]\n",
        "\n",
        "def FeatureExtract(data_idx,drug_feature,mutation_feature,gexpr_feature,methylation_feature):\n",
        "    cancer_type_list = []\n",
        "    nb_instance = len(data_idx)\n",
        "    nb_mutation_feature = mutation_feature.shape[1]\n",
        "    nb_gexpr_features = gexpr_feature.shape[1]\n",
        "    nb_methylation_features = methylation_feature.shape[1]\n",
        "    drug_data = [[] for item in range(nb_instance)]\n",
        "    mutation_data = np.zeros((nb_instance,1, nb_mutation_feature,1),dtype='float32')\n",
        "    gexpr_data = np.zeros((nb_instance,nb_gexpr_features),dtype='float32')\n",
        "    methylation_data = np.zeros((nb_instance, nb_methylation_features),dtype='float32')\n",
        "    target = np.zeros(nb_instance,dtype='float32')\n",
        "    for idx in range(nb_instance):\n",
        "        cell_line_id,pubchem_id,ln_IC50,cancer_type = data_idx[idx]\n",
        "        #modify\n",
        "        feat_mat,adj_list,_ = drug_feature[str(pubchem_id)]\n",
        "        #fill drug data,padding to the same size with zeros\n",
        "        drug_data[idx] = CalculateGraphFeat(feat_mat,adj_list)\n",
        "        #randomlize X A\n",
        "        mutation_data[idx,0,:,0] = mutation_feature.loc[cell_line_id].values\n",
        "        gexpr_data[idx,:] = gexpr_feature.loc[cell_line_id].values\n",
        "        methylation_data[idx,:] = methylation_feature.loc[cell_line_id].values\n",
        "        target[idx] = ln_IC50\n",
        "        cancer_type_list.append([cancer_type,cell_line_id,pubchem_id])\n",
        "    print(\"Feature Extract\")\n",
        "    return drug_data,mutation_data,gexpr_data,methylation_data,target,cancer_type_list\n",
        "\n",
        "class MyCallback(Callback):\n",
        "    def __init__(self,validation_data,patience):\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "        self.best_weight = None\n",
        "        self.patience = patience\n",
        "    def on_train_begin(self,logs={}):\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = -np.Inf\n",
        "        return\n",
        "    def on_train_end(self, logs={}):\n",
        "        self.model.set_weights(self.best_weight)\n",
        "        self.model.save('../checkpoint/MyBestDeepCDR_%s.h5'%model_suffix)\n",
        "        if self.stopped_epoch > 0 :\n",
        "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        pcc_val = pearsonr(self.y_val, y_pred_val[:,0])[0]\n",
        "        print ('pcc-val: %s' % str(round(pcc_val,4)))\n",
        "        if pcc_val > self.best:\n",
        "            self.best = pcc_val\n",
        "            self.wait = 0\n",
        "            self.best_weight = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait+=1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "        return\n",
        "def ModelTraining(model,X_drug_data_train,X_mutation_data_train,X_gexpr_data_train,X_methylation_data_train,Y_train,validation_data,nb_epoch=100):\n",
        "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "    model.compile(optimizer = optimizer,loss='mean_squared_error',metrics=['mse'])\n",
        "    #EarlyStopping(monitor='val_loss',patience=5)\n",
        "    callbacks = [ModelCheckpoint('../checkpoint/best_DeepCDR_%s.h5'%model_suffix,monitor='val_loss',save_best_only=False, save_weights_only=False),\n",
        "                MyCallback(validation_data=validation_data,patience=10)]\n",
        "    X_drug_feat_data_train = [item[0] for item in X_drug_data_train]\n",
        "    X_drug_adj_data_train = [item[1] for item in X_drug_data_train]\n",
        "    X_drug_feat_data_train = np.array(X_drug_feat_data_train)#nb_instance * Max_stom * feat_dim\n",
        "    X_drug_adj_data_train = np.array(X_drug_adj_data_train)#nb_instance * Max_stom * Max_stom\n",
        "    #validation data\n",
        "    model.fit(x=[X_drug_feat_data_train,X_drug_adj_data_train,X_mutation_data_train,X_gexpr_data_train,X_methylation_data_train],y=Y_train,batch_size=64,epochs=nb_epoch,validation_split=0,callbacks=callbacks)\n",
        "    return model\n",
        "\n",
        "\n",
        "def ModelEvaluate(model,X_drug_data_test,X_mutation_data_test,X_gexpr_data_test,X_methylation_data_test,Y_test,cancer_type_test_list,file_path):\n",
        "    X_drug_feat_data_test = [item[0] for item in X_drug_data_test]\n",
        "    X_drug_adj_data_test = [item[1] for item in X_drug_data_test]\n",
        "    X_drug_feat_data_test = np.array(X_drug_feat_data_test)#nb_instance * Max_stom * feat_dim\n",
        "    X_drug_adj_data_test = np.array(X_drug_adj_data_test)#nb_instance * Max_stom * Max_stom\n",
        "    Y_pred = model.predict([X_drug_feat_data_test,X_drug_adj_data_test,X_mutation_data_test,X_gexpr_data_test,X_methylation_data_test])\n",
        "    overall_pcc = pearsonr(Y_pred[:,0],Y_test)[0]\n",
        "    print(\"The overall Pearson's correlation is %.4f.\"%overall_pcc)\n",
        "\n",
        "\n",
        "def main():\n",
        "    random.seed(0)\n",
        "    mutation_feature, drug_feature,gexpr_feature,methylation_feature, data_idx = MetadataGenerate(Drug_info_file,Cell_line_info_file,Genomic_mutation_file,Drug_feature_file,Gene_expression_file,Methylation_file,False)\n",
        "    data_train_idx,data_test_idx = DataSplit(data_idx)\n",
        "    #Extract features for training and test\n",
        "    X_drug_data_train,X_mutation_data_train,X_gexpr_data_train,X_methylation_data_train,Y_train,cancer_type_train_list = FeatureExtract(data_train_idx,drug_feature,mutation_feature,gexpr_feature,methylation_feature)\n",
        "    X_drug_data_test,X_mutation_data_test,X_gexpr_data_test,X_methylation_data_test,Y_test,cancer_type_test_list = FeatureExtract(data_test_idx,drug_feature,mutation_feature,gexpr_feature,methylation_feature)\n",
        "\n",
        "    X_drug_feat_data_test = [item[0] for item in X_drug_data_test]\n",
        "    X_drug_adj_data_test = [item[1] for item in X_drug_data_test]\n",
        "    X_drug_feat_data_test = np.array(X_drug_feat_data_test)#nb_instance * Max_stom * feat_dim\n",
        "    X_drug_adj_data_test = np.array(X_drug_adj_data_test)#nb_instance * Max_stom * Max_stom\n",
        "\n",
        "    validation_data = [[X_drug_feat_data_test,X_drug_adj_data_test,X_mutation_data_test,X_gexpr_data_test,X_methylation_data_test],Y_test]\n",
        "    model = KerasMultiSourceGCNModel(use_mut,use_gexp,use_methy).createMaster(X_drug_data_train[0][0].shape[-1],X_mutation_data_train.shape[-2],X_gexpr_data_train.shape[-1],X_methylation_data_train.shape[-1],args.unit_list,args.use_relu,args.use_bn,args.use_GMP)\n",
        "    print('Begin training...')\n",
        "    model = ModelTraining(model,X_drug_data_train,X_mutation_data_train,X_gexpr_data_train,X_methylation_data_train,Y_train,validation_data,nb_epoch=50)\n",
        "    ModelEvaluate(model,X_drug_data_test,X_mutation_data_test,X_gexpr_data_test,X_methylation_data_test,Y_test,cancer_type_test_list,'%s/DeepCDR_%s.log'%(DPATH,model_suffix))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AeHg8sz3371t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdd9c3a-db73-4679-e41f-3e62ad319ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1441 instances across 558 cell lines and 4 drugs were generated.\n",
            "Feature Extract\n",
            "Feature Extract\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin training...\n",
            "Epoch 1/50\n",
            "9/9 [==============================] - 1s 18ms/step\n",
            "pcc-val: 0.4431\n",
            "16/16 [==============================] - 13s 137ms/step - loss: 5.1582 - mse: 5.1582\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "pcc-val: 0.5828\n",
            "16/16 [==============================] - 1s 71ms/step - loss: 2.9616 - mse: 2.9616\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "pcc-val: 0.6077\n",
            "16/16 [==============================] - 1s 74ms/step - loss: 2.5474 - mse: 2.5474\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 9ms/step\n",
            "pcc-val: 0.6303\n",
            "16/16 [==============================] - 1s 75ms/step - loss: 2.1453 - mse: 2.1453\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "pcc-val: 0.6268\n",
            "16/16 [==============================] - 1s 74ms/step - loss: 1.8498 - mse: 1.8498\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "pcc-val: 0.6459\n",
            "16/16 [==============================] - 1s 70ms/step - loss: 1.8337 - mse: 1.8337\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "pcc-val: 0.6315\n",
            "16/16 [==============================] - 1s 73ms/step - loss: 1.6983 - mse: 1.6983\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "pcc-val: 0.575\n",
            "16/16 [==============================] - 1s 83ms/step - loss: 1.4705 - mse: 1.4705\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 12ms/step\n",
            "pcc-val: 0.4823\n",
            "16/16 [==============================] - 1s 84ms/step - loss: 1.3591 - mse: 1.3591\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "pcc-val: 0.5365\n",
            "16/16 [==============================] - 1s 86ms/step - loss: 1.2825 - mse: 1.2825\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "pcc-val: 0.4906\n",
            "16/16 [==============================] - 1s 84ms/step - loss: 1.0377 - mse: 1.0377\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 12ms/step\n",
            "pcc-val: 0.4307\n",
            "16/16 [==============================] - 1s 76ms/step - loss: 1.0749 - mse: 1.0749\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 11ms/step\n",
            "pcc-val: 0.4613\n",
            "16/16 [==============================] - 1s 84ms/step - loss: 0.9883 - mse: 0.9883\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "pcc-val: 0.4686\n",
            "16/16 [==============================] - 1s 88ms/step - loss: 0.9318 - mse: 0.9318\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 10ms/step\n",
            "pcc-val: 0.4427\n",
            "16/16 [==============================] - 1s 83ms/step - loss: 0.8321 - mse: 0.8321\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 7ms/step\n",
            "pcc-val: 0.4549\n",
            "16/16 [==============================] - 1s 79ms/step - loss: 0.8342 - mse: 0.8342\n",
            "Epoch 00016: early stopping\n",
            "9/9 [==============================] - 0s 8ms/step\n",
            "The overall Pearson's correlation is 0.6459.\n"
          ]
        }
      ]
    }
  ]
}